{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-10T19:46:49.972648Z","iopub.status.busy":"2024-01-10T19:46:49.972222Z","iopub.status.idle":"2024-01-10T19:47:06.663851Z","shell.execute_reply":"2024-01-10T19:47:06.663023Z","shell.execute_reply.started":"2024-01-10T19:46:49.972615Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\LEGION\\anaconda3\\envs\\PRProject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import tqdm\n","import json\n","import random\n","import torch\n","import os\n","from collections import Counter \n","import datasets\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim.lr_scheduler import StepLR\n","from torch.optim import Adam\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","from transformers import AutoModelForQuestionAnswering, AutoTokenizer,Trainer,TrainingArguments\n","from sklearn.metrics import f1_score\n","import json"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:06.666529Z","iopub.status.busy":"2024-01-10T19:47:06.665905Z","iopub.status.idle":"2024-01-10T19:47:06.671254Z","shell.execute_reply":"2024-01-10T19:47:06.670385Z","shell.execute_reply.started":"2024-01-10T19:47:06.666500Z"},"trusted":true},"outputs":[],"source":["max_length = 256 # The maximum length of a feature (question and context)\n","doc_stride = 64 # The authorized overlap between two part of the context when splitting it is needed.\n","lr = 3e-5\n","\n","epoch = 30\n","batch_size = 4\n","model = \"C:/Users/LEGION/OneDrive - University Of Jordan/Dalalat/QA/QA_Model/\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:06.672649Z","iopub.status.busy":"2024-01-10T19:47:06.672376Z","iopub.status.idle":"2024-01-10T19:47:06.700722Z","shell.execute_reply":"2024-01-10T19:47:06.699869Z","shell.execute_reply.started":"2024-01-10T19:47:06.672626Z"},"trusted":true},"outputs":[],"source":["def f1_score(prediction_tokensIds, ground_truth_tokensIds):\n","    common = Counter(prediction_tokensIds) & Counter(ground_truth_tokensIds)\n","    num_same = sum(common.values())\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(prediction_tokensIds)\n","    recall = 1.0 * num_same / len(ground_truth_tokensIds)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","def pAP_score(mScores, ranks, gold_spans_set):\n","    \"\"\" Computing partial average precision \"\"\"\n","    score = 0.0\n","    partialHits = 0.0\n","    for mScore, rank in zip(mScores, ranks):\n","        if mScore != 0:\n","            partialHits = partialHits + mScore\n","            score += partialHits / rank\n","    return score / len(gold_spans_set) # pAP"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:06.702059Z","iopub.status.busy":"2024-01-10T19:47:06.701788Z","iopub.status.idle":"2024-01-10T19:47:11.635295Z","shell.execute_reply":"2024-01-10T19:47:11.634277Z","shell.execute_reply.started":"2024-01-10T19:47:06.702037Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","ar_tokenizer = AutoTokenizer.from_pretrained(model)\n","ar_model = AutoModelForQuestionAnswering.from_pretrained(model).to(device)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:11.637775Z","iopub.status.busy":"2024-01-10T19:47:11.637484Z","iopub.status.idle":"2024-01-10T19:47:11.652137Z","shell.execute_reply":"2024-01-10T19:47:11.650841Z","shell.execute_reply.started":"2024-01-10T19:47:11.637750Z"},"trusted":true},"outputs":[],"source":["def prepare_train_features(examples):\n","    global ar_tokenizer\n","    tokenized_examples = ar_tokenizer(\n","        examples[\"question\"],\n","        examples[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        \n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True)\n","    \n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","    tokenized_examples[\"start_positions\"] = []\n","    tokenized_examples[\"end_positions\"] = []\n","    \n","    for i, offsets in enumerate(offset_mapping):\n","        # We will label impossible answers with the index of the CLS token.\n","        input_ids = tokenized_examples[\"input_ids\"][i]\n","        cls_index = input_ids.index(ar_tokenizer.cls_token_id)\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        answers = examples[\"answers\"][sample_index]\n","        # If no answers are given, set the cls_index as answer.\n","        if len(answers[\"answer_start\"]) == 0:\n","            tokenized_examples[\"start_positions\"].append(cls_index)\n","            tokenized_examples[\"end_positions\"].append(cls_index)\n","        else:\n","            # Start/end character index of the answer in the text.\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","            # Start token index of the current span in the text.\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","            # End token index of the current span in the text.\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                tokenized_examples[\"start_positions\"].append(cls_index)\n","                tokenized_examples[\"end_positions\"].append(cls_index)\n","            else:\n","                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n","                # Note: we could go after the last offset if the answer is the last word (edge case).\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n","\n","    return tokenized_examples"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:11.653855Z","iopub.status.busy":"2024-01-10T19:47:11.653496Z","iopub.status.idle":"2024-01-10T19:47:11.852433Z","shell.execute_reply":"2024-01-10T19:47:11.851329Z","shell.execute_reply.started":"2024-01-10T19:47:11.653830Z"},"trusted":true},"outputs":[],"source":["def create_dataset(train_passage_question_objects):\n","    datasets_ = []\n","    for passage_question_object in train_passage_question_objects:\n","        for r in passage_question_object[\"answers\"]:\n","            # print(r)\n","            ans = dict({'answer_start': [r[\"start_char\"]], 'text': [r[\"text\"]]})\n","            datasets_.append(\n","                dict(\n","                {\"id\": passage_question_object[\"pq_id\"],\n","                \"context\": passage_question_object[\"passage\"],\n","                \"question\":passage_question_object[\"question\"],\n","                \"answers\": ans\n","                    }))\n","\n","    \n","    datasets_ = pd.DataFrame(datasets_)\n","    train_dataset = datasets.Dataset.from_dict(datasets_)\n","    return train_dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:11.854326Z","iopub.status.busy":"2024-01-10T19:47:11.854008Z","iopub.status.idle":"2024-01-10T19:47:11.869977Z","shell.execute_reply":"2024-01-10T19:47:11.868851Z","shell.execute_reply.started":"2024-01-10T19:47:11.854290Z"},"trusted":true},"outputs":[],"source":["def load_jsonl(input_path) -> list:\n","    \"\"\"\n","    Read list of objects from a JSON lines file.\n","    \"\"\"\n","    data = []\n","    with open(input_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            data.append(json.loads(line.rstrip('\\n|\\r')))\n","    print('Loaded {} records from {}'.format(len(data), input_path))\n","    return data\n","\n","def dump_jsonl(data, output_path, append=False):\n","    \"\"\"\n","    Write list of objects to a JSON lines file.\n","    \"\"\"\n","    mode = 'a+' if append else 'w'\n","    with open(output_path, mode, encoding='utf-8') as f:\n","        for line in data:\n","            json_record = json.dumps(line, ensure_ascii=False)\n","            f.write(json_record + '\\n')\n","    print('Wrote {} records to {}'.format(len(data), output_path))\n","\n","class Answer():\n","    def __init__(self,dictionary) -> None:\n","        self.text = dictionary[\"text\"]\n","        self.start_char = dictionary[\"start_char\"]\n","\n","    def to_dict(self) -> dict:\n","        answer_dict = {\n","        \"text\":self.text,\n","        \"start_char\":self.start_char\n","        }\n","        return answer_dict\n","\n","class PassageQuestion():\n","    def __init__(self,dictionary) -> None:\n","        self.pq_id = None\n","        self.passage = None\n","        self.surah = None\n","        self.verses = None\n","        self.question = None\n","        self.answers = []\n","        self.pq_id = dictionary[\"pq_id\"]\n","        self.passage = dictionary[\"passage\"]\n","        self.surah = dictionary[\"surah\"]\n","        self.verses = dictionary[\"verses\"]\n","        self.question = dictionary[\"question\"]\n","        for answer in dictionary[\"answers\"]:\n","            self.answers.append(Answer(answer))\n","\n","    def to_dict(self) -> dict:\n","        passge_question_dict = {\n","        \"pq_id\":self.pq_id,\n","        \"passage\":self.passage,\n","        \"surah\":self.surah,\n","        \"verses\":self.verses,\n","        \"question\":self.question,\n","        \"answers\":[x.to_dict() for x in self.answers]\n","        }\n","        return passge_question_dict\n","\n","def read_JSONL_file(file_path) -> list:\n","    data_in_file = load_jsonl(file_path)\n","\n","    # get list of PassageQuestion objects\n","    passage_question_objects = []\n","    for passage_question_dict in data_in_file:\n","        # instantiate a PassageQuestion object\n","        pq_object = PassageQuestion(passage_question_dict)\n","        print (f\"pq_id: {pq_object.pq_id}\")\n","        passage_question_objects.append(pq_object)\n","\n","    print(f\"Collected {len(passage_question_objects)} Object from {file_path}\")\n","    return passage_question_objects\n","\n","def write_to_JSONL_file(passage_question_objects,output_path) -> None:\n","\n","    # list of dictionaries for the passage_question_objects\n","    dict_data_list = []\n","    for pq_object in passage_question_objects:\n","        dict_data = pq_object.to_dict()\n","        dict_data_list.append(dict_data)\n","    dump_jsonl(dict_data_list,output_path)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:11.871507Z","iopub.status.busy":"2024-01-10T19:47:11.871189Z","iopub.status.idle":"2024-01-10T19:47:11.884557Z","shell.execute_reply":"2024-01-10T19:47:11.883799Z","shell.execute_reply.started":"2024-01-10T19:47:11.871482Z"},"trusted":true},"outputs":[],"source":["def train_QA():\n","    print(device)\n","    print(model)\n","    train_set_file = \"../QA/TaskB Data/QQA23_TaskB_qrcd_v1.2_train_preprocessed.jsonl\"\n","    dev_set_file = \"../QA/TaskB Data/QQA23_TaskB_qrcd_v1.2_dev_preprocessed.jsonl\"\n","\n","    train_passage_question_objects  = load_jsonl(train_set_file)\n","    dev_passage_question_objects = load_jsonl(dev_set_file)\n","\n","    train_dataset = create_dataset(train_passage_question_objects)\n","    dev_dataset = create_dataset(dev_passage_question_objects)\n","    my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset, \"dev\" : dev_dataset})\n","    tokenized_ds = my_dataset_dict.map(prepare_train_features, batched=True, remove_columns=my_dataset_dict[\"train\"].column_names)\n","\n","    args = TrainingArguments(\n","        f\"result\",\n","        evaluation_strategy = \"steps\",\n","        learning_rate=lr,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        num_train_epochs=epoch,\n","        weight_decay=0.0001,\n","        save_strategy = \"steps\",\n","        save_steps=1500,\n","        )\n","\n","\n","    trainer = Trainer(\n","    model=ar_model,\n","    args=args,\n","    train_dataset=tokenized_ds['train'],\n","    eval_dataset=tokenized_ds['dev'],\n","    tokenizer=ar_tokenizer)\n","\n","    # start training\n","    trainer.train()\n","    \n","    model_path = \"/kaggle/working/\"\n","    trainer.save_model(model_path)\n","    \n","    return ar_tokenizer, ar_model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T19:47:11.886140Z","iopub.status.busy":"2024-01-10T19:47:11.885763Z","iopub.status.idle":"2024-01-10T20:04:00.198898Z","shell.execute_reply":"2024-01-10T20:04:00.197016Z","shell.execute_reply.started":"2024-01-10T19:47:11.886106Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA\n","Loaded 992 records from /kaggle/input/taskb-data/QQA23_TaskB_qrcd_v1.2_train_preprocessed.jsonl\n","Loaded 163 records from /kaggle/input/taskb-data/TaskB Data/QQA23_TaskB_qrcd_v1.2_dev_preprocessed.jsonl\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1cb298920b14529893c1f9e57c08b51","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4525d2fc842f47aa8c6cc6a1ca33840d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.2 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240110_194719-pm70r34o</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/abd0205589/huggingface/runs/pm70r34o' target=\"_blank\">trim-smoke-6</a></strong> to <a href='https://wandb.ai/abd0205589/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/abd0205589/huggingface' target=\"_blank\">https://wandb.ai/abd0205589/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/abd0205589/huggingface/runs/pm70r34o' target=\"_blank\">https://wandb.ai/abd0205589/huggingface/runs/pm70r34o</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8610' max='8610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8610/8610 16:08, Epoch 30/30]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.612600</td>\n","      <td>2.452021</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.834900</td>\n","      <td>3.481510</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.646700</td>\n","      <td>3.977406</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.535100</td>\n","      <td>4.208948</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.480700</td>\n","      <td>4.633619</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.437500</td>\n","      <td>4.846795</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.421900</td>\n","      <td>4.844057</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.363100</td>\n","      <td>5.582001</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.391400</td>\n","      <td>5.571537</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.324200</td>\n","      <td>5.555631</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.335100</td>\n","      <td>5.913908</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.306600</td>\n","      <td>5.916940</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.299800</td>\n","      <td>6.065909</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.296900</td>\n","      <td>5.826698</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.304800</td>\n","      <td>5.892395</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.294200</td>\n","      <td>5.858017</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.286900</td>\n","      <td>5.845113</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(ElectraTokenizerFast(name_or_path='ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA', vocab_size=64000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n"," \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n"," \t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n"," \t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n"," \t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n"," \t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n"," },\n"," ElectraForQuestionAnswering(\n","   (electra): ElectraModel(\n","     (embeddings): ElectraEmbeddings(\n","       (word_embeddings): Embedding(64000, 768, padding_idx=0)\n","       (position_embeddings): Embedding(512, 768)\n","       (token_type_embeddings): Embedding(2, 768)\n","       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","       (dropout): Dropout(p=0.1, inplace=False)\n","     )\n","     (encoder): ElectraEncoder(\n","       (layer): ModuleList(\n","         (0-11): 12 x ElectraLayer(\n","           (attention): ElectraAttention(\n","             (self): ElectraSelfAttention(\n","               (query): Linear(in_features=768, out_features=768, bias=True)\n","               (key): Linear(in_features=768, out_features=768, bias=True)\n","               (value): Linear(in_features=768, out_features=768, bias=True)\n","               (dropout): Dropout(p=0.1, inplace=False)\n","             )\n","             (output): ElectraSelfOutput(\n","               (dense): Linear(in_features=768, out_features=768, bias=True)\n","               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","               (dropout): Dropout(p=0.1, inplace=False)\n","             )\n","           )\n","           (intermediate): ElectraIntermediate(\n","             (dense): Linear(in_features=768, out_features=3072, bias=True)\n","             (intermediate_act_fn): GELUActivation()\n","           )\n","           (output): ElectraOutput(\n","             (dense): Linear(in_features=3072, out_features=768, bias=True)\n","             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","             (dropout): Dropout(p=0.1, inplace=False)\n","           )\n","         )\n","       )\n","     )\n","   )\n","   (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n"," ))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_QA()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 992 records from ../QA/TaskB Data/QQA23_TaskB_qrcd_v1.2_train_preprocessed.jsonl\n"]}],"source":["train_set_file = \"../QA/TaskB Data/QQA23_TaskB_qrcd_v1.2_train_preprocessed.jsonl\"\n","\n","\n","train_passage_question_objects  = load_jsonl(train_set_file)\n","train_dataset = create_dataset(train_passage_question_objects)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["{'pq_id': '2:1-5_372',\n"," 'passage': 'الم . ذلك الكتاب لا ريب فيه هدى للمتقين . الذين يؤمنون بالغيب ويقيمون الصلاة ومما رزقناهم ينفقون . والذين يؤمنون بما أنزل إليك وما أنزل من قبلك وبالآخرة هم يوقنون . أولئك على هدى من ربهم وأولئك هم المفلحون .',\n"," 'surah': 2,\n"," 'verses': '1-5',\n"," 'question': 'ما الدلائل على أن القرآن ليس من تأليف سيدنا محمد ( ص ) ؟',\n"," 'answers': [{'text': 'الذين يؤمنون بما أنزل إليك وما أنزل من قبلك',\n","   'start_char': 100}]}"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["train_passage_question_objects[0]"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["a=ar_tokenizer.encode_plus(\n","    train_passage_question_objects[0]['question'],\n","    train_passage_question_objects[0]['passage'],\n","    max_length=256,\n","    padding='max_length',\n","    return_tensors=\"pt\",\n","    add_special_tokens=True,\n","    return_attention_mask=True\n",")"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[    2,   394, 34855,   323,   331,  4544,  1117,   306,  8315, 30404,\n","           582,    14,   126,    15,   105,     3,   301,    20,   563,  2886,\n","           391, 16072,   903, 13576,  6897,  3945,    20,   860, 28174,  4779,\n","           739, 27418,   319,  7938, 13626,  9754, 27147, 27207,   319,    20,\n","          6767, 28174,  1199, 34767, 20007,  1177, 34767,   306,   600,   209,\n","          2609,  4475,   197,  1891, 55118,  4817,    20, 10331,   323, 13576,\n","           306, 14756,   201, 53990,  1891,  2327,   182,  7653,    20,     3,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["a.to('cuda')"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["preds=ar_model(**a)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'QuestionAnsweringModelOutput' object has no attribute 'label_ids'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m squad_labels \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mlabel_ids\n","\u001b[1;31mAttributeError\u001b[0m: 'QuestionAnsweringModelOutput' object has no attribute 'label_ids'"]}],"source":["squad_labels = preds.label_ids"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4283938,"sourceId":7372940,"sourceType":"datasetVersion"},{"datasetId":4286807,"sourceId":7377868,"sourceType":"datasetVersion"},{"datasetId":4273709,"sourceId":7357957,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
